{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference for Competition \"Don't Overfit\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data Analysis\n",
    "This data analysis partly referenced a hot kernel of this competition on kaggle: [Tutorial on Esemble Learning](https://www.kaggle.com/mjbahmani/tutorial-on-ensemble-learning-don-t-overfit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "train_data = pd.read_csv(\"./Data/train.csv\")\n",
    "test_data = pd.read_csv(\"./Data/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label Distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1411a608eb8>"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['target'].value_counts().plot.bar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quite obvious class-imbalance can be spotted above, which means we might want to deal with this problem later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Distribution:\n",
    "According to the evaluation results of the [kernel](https://www.kaggle.com/mjbahmani/tutorial-on-ensemble-learning-don-t-overfit) mentioned above, the distributions of almost all the feautures (even when they are mixed together) follow shapes similar to that of gaussian distribution. _It might make sense to compare the distributions of each feature based on their label (target). Considering that I have quite limited time for this game, I write down these sentences in Italian and it might remind me of that sometime later._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train/test data\n",
    "from sklearn.model_selection import train_test_split\n",
    "X = train_data.drop([\"target\",\"id\"], axis=1)\n",
    "Y = train_data[\"target\"]\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
    "X_test = test_data.drop(\"id\", axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Model Selection\n",
    "The author of the [kernel](https://www.kaggle.com/mjbahmani/tutorial-on-ensemble-learning-don-t-overfit) listed above mentioned a category of ensemble models (techniques), but I kind of want to stop following his step and try some models I'm interested in. Of course bagging and boosting are something to try since they are so intuitive when talking about __\"overfit\"__. Additionally I might also try SVM, GD (gradient descent) and their variants. During the following paragraphs I'll discuss __(review)__ these algorithms or models one by one and try them out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Bagging, Boosting and Comparison\n",
    "First of all, around 80% of all my (future) analysis would be based on or directly coming from the course [Machine Learning for Intelligent Systems](http://www.cs.cornell.edu/courses/cs4780/2018fa/) I took 2018 Fall provided by professor [Kilian Q. Weinberger](http://kilian.cs.cornell.edu/) at Cornell University; 50% of the remaining would be based on the course [Principles of Large Scale Machine Learning](http://www.cs.cornell.edu/courses/cs4780/2018fa/) I'm currently (2019 SP) enrolling in provided by professor Christopher De Sa at CU. I'm writing this because I don't want to have trouble about copyright or something(of course I will not)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bagging\n",
    "The goal of using bagging is usually reduce the __variance__ of a model, which generally means the error cause by dataset with limited representing capability of the whole distribution we assume our data have (which is in most cases true). In other words, variance exists because the model (classifier) is too \"specialized\" about the dataset we provide, which cause overfit.  \n",
    "  \n",
    "The intuition that Bagging will work is based on the __Weak Law of Large Numbers__, which generally says states that for i.i.d radom variables sampled from a certain distribution, as the number of these random variables approach infinity, their mean can be a good approximation of the estimation of this distribution.    \n",
    "\n",
    "Based on this law, people try to \"procude\" as many as possible \"datasets\" to train as many as possible classifiers, so that the average of these classifers can be a good approximation of the ultimate model we are looking for. The way to produce many \"datasets\" is sampling (with replacement) from the only datasets we have.  \n",
    "  \n",
    "  Implement the most famous Bagging variant Random Forest Classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        19\n",
      "         1.0       1.00      1.00      1.00        31\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        50\n",
      "   macro avg       1.00      1.00      1.00        50\n",
      "weighted avg       1.00      1.00      1.00        50\n",
      "\n",
      "Confusion Matrix:\n",
      " [[19  0]\n",
      " [ 0 31]]\n",
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as RandomForest\n",
    "\n",
    "\"\"\"\n",
    "as for model parameters, max depth is set to default, which you can also say\n",
    "infinity because it will split until all leaves are pure, which overfits\n",
    "hell. On the other hand, I'll try to have as many trees as possible.\n",
    "The number of features to consider for each split is square root of the number\n",
    "of all features.\n",
    "\"\"\"\n",
    "cls = RandomForest(n_estimators=100, max_features=\"sqrt\")\n",
    "cls.fit(X, Y)\n",
    "y_pred = cls.predict(X_val)\n",
    "print(classification_report(y_pred, Y_val))\n",
    "print(\"Confusion Matrix:\\n\",confusion_matrix(y_pred, Y_val))\n",
    "print(\"Accuracy: {}\".format(accuracy_score(y_pred, Y_val)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create submission file\n",
    "prediction = cls.predict(X_test)\n",
    "result = pd.DataFrame(prediction, columns=[\"target\"], dtype=int)\n",
    "result.index = range(250,20000)\n",
    "result.to_csv(\"./data/result.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
